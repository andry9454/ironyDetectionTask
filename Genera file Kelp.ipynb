{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \n",
       "0      0        0   HSC  \n",
       "1      0        0   HSC  \n",
       "2      0        0   HSC  "
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, irony, sarcasm, topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "df[(df.irony==0) & (df.sarcasm ==1)].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>#Risorse da accogliere... Omicidi e stupri nel campo #profughi in #Libia: arrestato 22enne somalo a #Milano. https://t.co/Y2CO7y0UbU</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Uau mario monti ci fa il regalo dinatale e approda su twitter. Che culo.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>Chi è entrato nel cellulare di Jennifer Lawrence potrebbe entrare anche nel tuo. E troverebbe sempre le foto della Lawrence. [@TheAubergine]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>“la felicità non ha prezzo“, la mia sì, costa circa 3,50€ e si chiama Nutella. #nonpuoimancare</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>Ebola, il medico italiano guarisce grazie a una cura sperimentale. Che ora verrà testata sui vigili di Roma. [@ZipSatiraLampo]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Se qualcuno di voi mi dimostra che negri, rom, islamici e clandestini sono davvero risorse, GIURO CHE BACIO IL CULO ALLA KYENGE!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3637</th>\n",
       "      <td>@GaspErCarbonaro #labuonascuola del Gelataio è anche questa meglio che imparino da subito. A tutti in omaggio Oliver Twist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>delocalizziamo alla Libia la gestione dei migranti, ottimo https://t.co/8ZYXQy9a9l</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>@forummediaset dite alla Contessa che i rom hanno macchinoni e oro a cianfe perchè LI RUBANO, non certo perché lavorano onestAmente! #schifo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>Ora che sono stati scoperti nuovi pianeti potreste anche mandarci i migranti. Sarebbe un mondo tutto per loro.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>È morto l'uomo più grasso del mondo. Peccato perché dal 7 gennaio sarebbe andato in palestra. [@pirata_21]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Ma come cazzo dormi la notte Nessuno rom vuole fare gli straordinari? Magari gli incursori dell'Est Qualche profugo in cerca di riparo?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>@scuolapd @pdnetwork La buona scuola prevede anche bagni funzionanti e puliti?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>La Severino dice che il suo portinaio la incita ad andare avanti...ti credo sennò chi gliela da la mancia a natale? #manovra #governo #monti</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Metal detector in Vaticano. Finalmente li vedremo svuotare le tasche.  [CONTINUA su https://t.co/oDPUtx2DvV]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>Certo che in #Tunisia non dev'essere facile indagare su estremismo islamico. Sono tutti estremisti</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>@MrEfis79 @KingArcobaleno Oh ma tu sei quello tutto processo e codice penale ma chi difende i rom zingari clandestini? siete voi caro</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>Irresponsabile far cadere governo Monti . Mister B deve stare proprio male per fare certe dichiarazioni. Povero :-(</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>#primogiornodiscuola, quindicenne cade dal quinto piano della scuola. Era assente quando hanno spiegato la gravità. #labuonascuolauncazzo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>Ma i punti de #labuonascuola sono riferiti anche a quella pubblica? #perdire</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              text  \\\n",
       "2668          #Risorse da accogliere... Omicidi e stupri nel campo #profughi in #Libia: arrestato 22enne somalo a #Milano. https://t.co/Y2CO7y0UbU   \n",
       "117                                                                       Uau mario monti ci fa il regalo dinatale e approda su twitter. Che culo.   \n",
       "2236  Chi è entrato nel cellulare di Jennifer Lawrence potrebbe entrare anche nel tuo. E troverebbe sempre le foto della Lawrence. [@TheAubergine]   \n",
       "3878                                                “la felicità non ha prezzo“, la mia sì, costa circa 3,50€ e si chiama Nutella. #nonpuoimancare   \n",
       "1974                Ebola, il medico italiano guarisce grazie a una cura sperimentale. Che ora verrà testata sui vigili di Roma. [@ZipSatiraLampo]   \n",
       "324             Se qualcuno di voi mi dimostra che negri, rom, islamici e clandestini sono davvero risorse, GIURO CHE BACIO IL CULO ALLA KYENGE!!!   \n",
       "3637                 @GaspErCarbonaro #labuonascuola del Gelataio è anche questa meglio che imparino da subito. A tutti in omaggio Oliver Twist...   \n",
       "2078                                                            delocalizziamo alla Libia la gestione dei migranti, ottimo https://t.co/8ZYXQy9a9l   \n",
       "3662  @forummediaset dite alla Contessa che i rom hanno macchinoni e oro a cianfe perchè LI RUBANO, non certo perché lavorano onestAmente! #schifo   \n",
       "787                                 Ora che sono stati scoperti nuovi pianeti potreste anche mandarci i migranti. Sarebbe un mondo tutto per loro.   \n",
       "1994                                    È morto l'uomo più grasso del mondo. Peccato perché dal 7 gennaio sarebbe andato in palestra. [@pirata_21]   \n",
       "1126      Ma come cazzo dormi la notte Nessuno rom vuole fare gli straordinari? Magari gli incursori dell'Est Qualche profugo in cerca di riparo?    \n",
       "3199                                                                @scuolapd @pdnetwork La buona scuola prevede anche bagni funzionanti e puliti?   \n",
       "1218  La Severino dice che il suo portinaio la incita ad andare avanti...ti credo sennò chi gliela da la mancia a natale? #manovra #governo #monti   \n",
       "976                                   Metal detector in Vaticano. Finalmente li vedremo svuotare le tasche.  [CONTINUA su https://t.co/oDPUtx2DvV]   \n",
       "2258                                            Certo che in #Tunisia non dev'essere facile indagare su estremismo islamico. Sono tutti estremisti   \n",
       "3324         @MrEfis79 @KingArcobaleno Oh ma tu sei quello tutto processo e codice penale ma chi difende i rom zingari clandestini? siete voi caro   \n",
       "1431                           Irresponsabile far cadere governo Monti . Mister B deve stare proprio male per fare certe dichiarazioni. Povero :-(   \n",
       "2689     #primogiornodiscuola, quindicenne cade dal quinto piano della scuola. Era assente quando hanno spiegato la gravità. #labuonascuolauncazzo   \n",
       "1115                                                                  Ma i punti de #labuonascuola sono riferiti anche a quella pubblica? #perdire   \n",
       "\n",
       "      irony  sarcasm  \n",
       "2668      1        1  \n",
       "117       1        0  \n",
       "2236      1        0  \n",
       "3878      1        0  \n",
       "1974      1        0  \n",
       "324       1        1  \n",
       "3637      1        0  \n",
       "2078      1        1  \n",
       "3662      1        1  \n",
       "787       1        1  \n",
       "1994      1        0  \n",
       "1126      1        1  \n",
       "3199      1        1  \n",
       "1218      1        1  \n",
       "976       1        1  \n",
       "2258      1        1  \n",
       "3324      1        1  \n",
       "1431      1        0  \n",
       "2689      1        1  \n",
       "1115      1        1  "
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "df[(df.irony==1) | (df.sarcasm ==1)][['text','irony','sarcasm']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         2023\n",
       "text       2023\n",
       "irony      2023\n",
       "sarcasm    2023\n",
       "topic      2023\n",
       "dtype: int64"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.irony==1) | (df.sarcasm ==1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbashCommand = \"java -version\"\\nimport subprocess\\nprocess = subprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'], stdout=subprocess.PIPE)\\noutput, error = process.communicate()\\nprint(output)\\nprint(error)\\n\\nsubprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'])\\nfrom subprocess import check_output\\nout = check_output([\"java\", \"-version\"])\\nprint(out)\\n\\n'"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bashCommand = \"java -version\"\n",
    "import subprocess\n",
    "process = subprocess.Popen(['/bin/sh', 'java', '-version'], stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output)\n",
    "print(error)\n",
    "\n",
    "subprocess.Popen(['/bin/sh', 'java', '-version'])\n",
    "from subprocess import check_output\n",
    "out = check_output([\"java\", \"-version\"])\n",
    "print(out)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArraydataset(df=df, target=\"irony\", lower = True):\n",
    "    arrayDataset = []\n",
    "    arrayTarget = []\n",
    "    for index, row in df.iterrows():\n",
    "        if(lower):\n",
    "            arrayDataset.append(row[\"text\"].lower())\n",
    "        else:\n",
    "            arrayDataset.append(row[\"text\"])\n",
    "        if(row[target] == 0):\n",
    "            arrayTarget.append(-1)\n",
    "        else:\n",
    "            arrayTarget.append(row[target])\n",
    "    return [arrayDataset, arrayTarget]\n",
    "\n",
    "array = generaArraydataset()\n",
    "X_train = array[0]\n",
    "y_train_irony = array[1]\n",
    "y_train_sarcasm = generaArraydataset(target=\"sarcasm\")[1]\n",
    "\n",
    "'''\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "#token = nltk.word_tokenize(X_train[2])\n",
    "print(tknzr.tokenize(X_train[2]))\n",
    "'''\n",
    "print(y_train_irony[2])\n",
    "print(y_train_sarcasm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import hashlib\n",
    "\n",
    "def creaBoW(dataset, target, name = \"dataset.klp\"):\n",
    "    f = open(name,\"w+\")\n",
    "    for index, riga in enumerate(dataset):\n",
    "        string = str(target[index]) + \" |BV:bow| \"\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(dataset[index])\n",
    "        for word in tweettoknz:\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "        f.write(string + \"|EV| |BS:comment| \"+ dataset[index] +\" |ES| |BS:IDTweet| \"+ str(df.iloc[index]['id']) +\" |ES| \\n\")    \n",
    "    f.close()\n",
    "    \n",
    "NOME_FILE = \"dataset_ironia.klp\"\n",
    "#creaBoW(X_train, y_train_sarcasm, NOME_FILE)\n",
    "creaBoW(X_train, y_train_irony, NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "      <th>text::text::S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \\\n",
       "0      0        0   HSC   \n",
       "1      0        0   HSC   \n",
       "2      0        0   HSC   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                    text::text::S   \n",
       "0                                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL   \n",
       "1                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL   \n",
       "2  Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL   "
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018_renlt_processed.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArrayTokenizzatoLemmi(df=df):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    #per ogni elemento del dataset\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        #per ogni token text::text::S\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            #divido ogni singolo token e prendo il lemma\n",
    "            arrayFrase.append(arrayParola[1])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "\n",
    "def generaArrayTokenizzatoSurface(df=df, listPOS = ['A','B','S','V','HTG'], lower = True):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            if((arrayParola[2] in listPOS) and listPOS != []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "            if(listPOS == []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "    \n",
    "prova = generaArrayTokenizzatoLemmi()\n",
    "print prova[2]\n",
    "prova2 = generaArrayTokenizzatoSurface()\n",
    "print prova2[2]\n",
    "arrayCompleto = generaArrayTokenizzatoSurface(listPOS = [])\n",
    "print arrayCompleto[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generaArrayNgrams(dataset, ngram=2):\n",
    "    arrayBigrammi = []\n",
    "    for item in dataset:\n",
    "        arrayBigrammi.append(list(ngrams(item,ngram)))\n",
    "    return arrayBigrammi\n",
    "\n",
    "\n",
    "arrayBigrammiLemmi = generaArrayNgrams(prova)\n",
    "arrayBigrammiSurface = generaArrayNgrams(arrayCompleto)\n",
    "\n",
    "\n",
    "arrayBigrammiChar = generaArrayNgrams(X_train)\n",
    "array3grammiChar = generaArrayNgrams(X_train, 3)\n",
    "array4grammiChar = generaArrayNgrams(X_train, 4)\n",
    "array5grammiChar = generaArrayNgrams(X_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "def importaWordSpace(pathWS = \"../ws\"):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[3:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace2(pathWS = '../DPL-IT_lrec2016.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[1:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace3(pathWS = '../emoji2vec.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def generaCombWordSpace(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm != 0):\n",
    "            combDataset.append(comb/norm)\n",
    "        else:\n",
    "            combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "'''\n",
    "def getCentroide(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        i = 0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                i = i + 1\n",
    "        if(i == 0):\n",
    "            i = 1\n",
    "        combDataset.append(comb/i)\n",
    "    return combDataset\n",
    "'''\n",
    "def generaCombDPL(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace2()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(3)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm == 0):\n",
    "            norm = 1\n",
    "        combDataset.append(comb/norm)\n",
    "    return combDataset\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def generaEmoji(dataset = X_train):\n",
    "    embeddings_index = importaWordSpace3()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(tweet)\n",
    "        comb = np.zeros(300)\n",
    "        for parola in tweettoknz:\n",
    "            if(parola in embeddings_index):\n",
    "                print parola\n",
    "                comb = comb + embeddings_index[parola]\n",
    "        combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "def feturesPunteggiatura(dataset = X_train):\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        charset = [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "        numUpperCase = 0.0\n",
    "        numchar = 0.0\n",
    "        for char in tweet:\n",
    "            numchar += 1\n",
    "            if(char.istitle()):\n",
    "                numUpperCase += 1\n",
    "            if(char == \"!\"):\n",
    "                charset[0] += 1\n",
    "            if(char == \"?\"):\n",
    "                charset[1] += 1\n",
    "            if(char == \".\"):\n",
    "                charset[2] += 1    \n",
    "            if(char == \",\"):\n",
    "                charset[3] += 1\n",
    "            if(char == \";\"):\n",
    "                charset[4] += 1\n",
    "            if(char == \":\"):\n",
    "                charset[5] += 1\n",
    "        charset.append(numUpperCase/numchar)\n",
    "        combDataset.append(charset)\n",
    "    return combDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "#Combinazione Lineare delle parole del Word Space moltiplicate per la distanza dal centroide\n",
    "#Per ogni Elemento del dataset ritorna la tupla (Bow, Var, Mean)\n",
    "def generaIronySpecificBow(dataset, arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for index, tweet in enumerate(dataset):\n",
    "        comb = np.zeros(250)\n",
    "        arrayDistanze = []\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola] * distance.cosine(embeddings_index[parola], arrayCentroide[index])\n",
    "                arrayDistanze.append(distance.euclidean(embeddings_index[parola], arrayCentroide[index]))\n",
    "        x = asarray(arrayDistanze)\n",
    "        aList = (comb, np.var(x), np.mean(x))\n",
    "        combDataset.append(tuple(aList))\n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def appendBowIronySpecific(dataset, nomerappresentazione, file_name = \"dataset.klp\", embeddings_index = importaWordSpace(), arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    combDataset = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        arrayDistanze = []\n",
    "        for word in dataset[index]:\n",
    "            #calcola distanza tra word e centroide[index]\n",
    "            if(word in embeddings_index):\n",
    "                distanza = distance.cosine(arrayCentroide[index], embeddings_index[word])\n",
    "                arrayDistanze.append(distance.cosine(arrayCentroide[index], embeddings_index[word]))\n",
    "            else:\n",
    "                distanza = 1.0\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":\"+ str(distanza)+\" \"    \n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        x = asarray(arrayDistanze)\n",
    "        if(np.isnan(np.var(x)) or np.isnan(np.mean(x))):\n",
    "            aList = [0,0]\n",
    "        else:\n",
    "            aList = [np.var(x), np.mean(x)]\n",
    "        combDataset.append(aList)\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "    \n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def appendBow(dataset, nomerappresentazione, file_name = \"dataset.klp\", tupla = True):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        for word in dataset[index]:\n",
    "            if(tupla):\n",
    "                string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "            else:\n",
    "                string = string + \"_\" + word + \":1.0 \"\n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines) \n",
    "\n",
    "def appendDenseRapr(dataset, nomerappresentazione, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        for num in dataset[index]:\n",
    "            string = string + str(num) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "\n",
    "\n",
    "def appendDenseRaprTuple(dataset, nomerappresentazione, idTupla, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        if( idTupla < 1):\n",
    "            for num in dataset[index][idTupla]:\n",
    "                string = string + str(num) + \" \"\n",
    "        else:\n",
    "            string = string + str(dataset[index][idTupla]) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "combWS = generaCombWordSpace()\n",
    "appendDenseRapr(combWS, \"WSSurface\",NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "comDPL = generaCombDPL()\n",
    "appendDenseRapr(comDPL, \"combDPL\",NOME_FILE)\n",
    "\n",
    "combEmoji = generaEmoji()\n",
    "appendDenseRapr(combEmoji, \"combEmoji\",NOME_FILE)\n",
    "\n",
    "featuresPuntegg = feturesPunteggiatura()\n",
    "appendDenseRapr(featuresPuntegg, \"featPunt\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanVarArray = appendBowIronySpecific(prova, \"bowIronySpecific\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray, \"VarMean\",NOME_FILE)\n",
    "IronySpecificASV = generaArrayTokenizzatoSurface(listPOS = ['A','S','V'])\n",
    "meanVarArray2 = appendBowIronySpecific(IronySpecificASV, \"bowIronySpecificASV\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray2, \"VarMeanASV\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "irony_specific_bow = generaIronySpecificBow(arrayCompleto)\n",
    "appendDenseRaprTuple(irony_specific_bow, \"IronySpecificWS\", 0 ,NOME_FILE)\n",
    "#appendDenseRaprTuple(irony_specific_bow, \"Var\", 1 ,NOME_FILE)\n",
    "#appendDenseRaprTuple(irony_specific_bow, \"Mean\", 2 ,NOME_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendBow(prova, \"bowLemmi\", NOME_FILE, tupla = False)\n",
    "appendBow(arrayBigrammiLemmi, \"bowBigramLemmi\", NOME_FILE)\n",
    "appendBow(arrayBigrammiSurface, \"bowBigramSurface\", NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "appendBow(arrayBigrammiChar, \"bow2gramChar\", NOME_FILE)\n",
    "appendBow(array3grammiChar, \"bow3gramChar\", NOME_FILE)\n",
    "appendBow(array4grammiChar, \"bow4gramChar\", NOME_FILE)\n",
    "appendBow(array5grammiChar, \"bow5gramChar\", NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
