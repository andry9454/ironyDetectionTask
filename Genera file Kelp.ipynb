{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \n",
       "0      0        0   HSC  \n",
       "1      0        0   HSC  \n",
       "2      0        0   HSC  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, irony, sarcasm, topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "df[(df.irony==0) & (df.sarcasm ==1)].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Roma, Bertolaso porta 50.000 persone ai gazebo. All'Aquila c'era riuscito con i tendoni. [@leomorabito]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>Non abbiamo bisogno dei terroristi musu...riusciamo a farci male da soli! https://t.co/erUSlcXPwL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>Grecia, lunghe file fuori dalle banche. Ora chiunque può comprarne una. [@pantana21]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>muore qualcuno e improvvisamente tutti si ricordano della sua esistenza. #morosini</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814</th>\n",
       "      <td>@andreapoggioli Dalle mie parti ho visto un cartello: Monte Mario sì, Mario Monti no. #trailetueconclusioni.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>@francescocosta: «Che mi chiami Marione proprio non lo sopporto» (qualche aneddoto su Mario Monti) http://t.co/foic31ZS @bosconauta</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>Prima di dimettersi Marino stanzia dieci milioni per il Giubileo. L'assegno è intestato ad Ali Agca. [@_edelman]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>Ho letto #labuonascuola di #incoeRenzi:nessun progetto concreto! E' riuscito a fare il gelato senza latte ne aromi! #ilMagodellaFuffa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>@matteorenzi ti consiglio dopo #labuonascuola di creare con L'abolizione dell'art18 #ilbuonlavoro come e per #tfaordinario altri disoccupati</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Renzi visita tre fabbriche. Ora può dire di averle girate tutte. [@gmarco_g, @nicolakeplacid]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>con quali e (di chi) soldi intende mantenere Corradino Mineo tutti i profughi in Italia ?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>@repubblicait dite alla raggi di dare una casa anche a quei nomadi che hanno scippato la studentessa cinese</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Esce il film su papa Francesco. Preceduto dal corto su papa Luciani. [@Goemon_SM]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Una volta lo sciopero scolastico era al sabato, da quando c'è la settimana corta anche a scuola, si anticipa tutto al venerdì #labuonascuola</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>se la tua donna di estrema sinistra appoggia il governo #Monti ,ha simpatie per #bocchino cosa fare?è di buon auspicio o mi devo preoccupare</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>@gparagone: Perché Patroni Griffi è ancora nel governo #Monti? [a sua insaputa, I presume]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>e se il merito non convenisse? #labuonascuola :ovvero come ti pago meno i docenti! http://t.co/uwh9WUxvsg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>@FabrizioDalCol la comunita' islamica deve averlo pagato molto bene..........</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Servizio veloce e gratuito per gli immigrati dal mare libico a Lampedusa. Possono imbarcarsi anche i terroristi dell'Isis</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>L'attentatore di Tunisi aveva fatto l'animatore. Quindi non era la prima volta che portava il terrore in spiaggia. [@fdecollibus]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              text  \\\n",
       "499                                        Roma, Bertolaso porta 50.000 persone ai gazebo. All'Aquila c'era riuscito con i tendoni. [@leomorabito]   \n",
       "869                                              Non abbiamo bisogno dei terroristi musu...riusciamo a farci male da soli! https://t.co/erUSlcXPwL   \n",
       "1768                                                          Grecia, lunghe file fuori dalle banche. Ora chiunque può comprarne una. [@pantana21]   \n",
       "921                                                             muore qualcuno e improvvisamente tutti si ricordano della sua esistenza. #morosini   \n",
       "3814                                  @andreapoggioli Dalle mie parti ho visto un cartello: Monte Mario sì, Mario Monti no. #trailetueconclusioni.   \n",
       "3658           @francescocosta: «Che mi chiami Marione proprio non lo sopporto» (qualche aneddoto su Mario Monti) http://t.co/foic31ZS @bosconauta   \n",
       "669                               Prima di dimettersi Marino stanzia dieci milioni per il Giubileo. L'assegno è intestato ad Ali Agca. [@_edelman]   \n",
       "1740         Ho letto #labuonascuola di #incoeRenzi:nessun progetto concreto! E' riuscito a fare il gelato senza latte ne aromi! #ilMagodellaFuffa   \n",
       "3398  @matteorenzi ti consiglio dopo #labuonascuola di creare con L'abolizione dell'art18 #ilbuonlavoro come e per #tfaordinario altri disoccupati   \n",
       "557                                                  Renzi visita tre fabbriche. Ora può dire di averle girate tutte. [@gmarco_g, @nicolakeplacid]   \n",
       "2172                                                     con quali e (di chi) soldi intende mantenere Corradino Mineo tutti i profughi in Italia ?   \n",
       "3227                                   @repubblicait dite alla raggi di dare una casa anche a quei nomadi che hanno scippato la studentessa cinese   \n",
       "1942                                                             Esce il film su papa Francesco. Preceduto dal corto su papa Luciani. [@Goemon_SM]   \n",
       "82    Una volta lo sciopero scolastico era al sabato, da quando c'è la settimana corta anche a scuola, si anticipa tutto al venerdì #labuonascuola   \n",
       "331   se la tua donna di estrema sinistra appoggia il governo #Monti ,ha simpatie per #bocchino cosa fare?è di buon auspicio o mi devo preoccupare   \n",
       "3601                                                    @gparagone: Perché Patroni Griffi è ancora nel governo #Monti? [a sua insaputa, I presume]   \n",
       "1987                                     e se il merito non convenisse? #labuonascuola :ovvero come ti pago meno i docenti! http://t.co/uwh9WUxvsg   \n",
       "3692                                                                 @FabrizioDalCol la comunita' islamica deve averlo pagato molto bene..........   \n",
       "290                      Servizio veloce e gratuito per gli immigrati dal mare libico a Lampedusa. Possono imbarcarsi anche i terroristi dell'Isis   \n",
       "1401             L'attentatore di Tunisi aveva fatto l'animatore. Quindi non era la prima volta che portava il terrore in spiaggia. [@fdecollibus]   \n",
       "\n",
       "      irony  sarcasm  \n",
       "499       1        1  \n",
       "869       1        1  \n",
       "1768      1        0  \n",
       "921       1        0  \n",
       "3814      1        0  \n",
       "3658      1        0  \n",
       "669       1        1  \n",
       "1740      1        1  \n",
       "3398      1        1  \n",
       "557       1        0  \n",
       "2172      1        1  \n",
       "3227      1        1  \n",
       "1942      1        0  \n",
       "82        1        0  \n",
       "331       1        0  \n",
       "3601      1        0  \n",
       "1987      1        1  \n",
       "3692      1        1  \n",
       "290       1        1  \n",
       "1401      1        0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "df[(df.irony==1) | (df.sarcasm ==1)][['text','irony','sarcasm']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         2023\n",
       "text       2023\n",
       "irony      2023\n",
       "sarcasm    2023\n",
       "topic      2023\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.irony==1) | (df.sarcasm ==1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbashCommand = \"java -version\"\\nimport subprocess\\nprocess = subprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'], stdout=subprocess.PIPE)\\noutput, error = process.communicate()\\nprint(output)\\nprint(error)\\n\\nsubprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'])\\nfrom subprocess import check_output\\nout = check_output([\"java\", \"-version\"])\\nprint(out)\\n\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bashCommand = \"java -version\"\n",
    "import subprocess\n",
    "process = subprocess.Popen(['/bin/sh', 'java', '-version'], stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output)\n",
    "print(error)\n",
    "\n",
    "subprocess.Popen(['/bin/sh', 'java', '-version'])\n",
    "from subprocess import check_output\n",
    "out = check_output([\"java\", \"-version\"])\n",
    "print(out)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArraydataset(df=df, target=\"irony\", lower = True):\n",
    "    arrayDataset = []\n",
    "    arrayTarget = []\n",
    "    for index, row in df.iterrows():\n",
    "        if(lower):\n",
    "            arrayDataset.append(row[\"text\"].lower())\n",
    "        else:\n",
    "            arrayDataset.append(row[\"text\"])\n",
    "        if(row[target] == 0):\n",
    "            arrayTarget.append(-1)\n",
    "        else:\n",
    "            arrayTarget.append(row[target])\n",
    "    return [arrayDataset, arrayTarget]\n",
    "\n",
    "array = generaArraydataset()\n",
    "X_train = array[0]\n",
    "y_train_irony = array[1]\n",
    "y_train_sarcasm = generaArraydataset(target=\"sarcasm\")[1]\n",
    "\n",
    "'''\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "#token = nltk.word_tokenize(X_train[2])\n",
    "print(tknzr.tokenize(X_train[2]))\n",
    "'''\n",
    "print(y_train_irony[2])\n",
    "print(y_train_sarcasm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import hashlib\n",
    "\n",
    "def creaBoW(dataset, target, name = \"dataset.klp\"):\n",
    "    f = open(name,\"w+\")\n",
    "    for index, riga in enumerate(dataset):\n",
    "        string = str(target[index]) + \" |BV:bow| \"\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(dataset[index])\n",
    "        for word in tweettoknz:\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "        f.write(string + \"|EV| |BS:comment| \"+ dataset[index] +\" |ES| |BS:IDTweet| \"+ str(df.iloc[index]['id']) +\" |ES| \\n\")    \n",
    "    f.close()\n",
    "\n",
    "    \n",
    "NOME_FILE = \"dataset_ironia.klp\"\n",
    "creaBoW(X_train, y_train_irony, NOME_FILE)\n",
    "\n",
    "#NOME_FILE = \"dataset_sarcasmo.klp\"\n",
    "#creaBoW(X_train, y_train_sarcasm, NOME_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "      <th>text::text::S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \\\n",
       "0      0        0   HSC   \n",
       "1      0        0   HSC   \n",
       "2      0        0   HSC   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                    text::text::S   \n",
       "0                                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL   \n",
       "1                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL   \n",
       "2  Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL   "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018_renlt_processed.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArrayTokenizzatoLemmi(df=df):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    #per ogni elemento del dataset\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        #per ogni token text::text::S\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            #divido ogni singolo token e prendo il lemma\n",
    "            arrayFrase.append(arrayParola[1])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "\n",
    "def generaArrayTokenizzatoSurface(df=df, listPOS = ['A','B','S','V','HTG'], lower = True):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            if((arrayParola[2] in listPOS) and listPOS != []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "            if(listPOS == []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "    \n",
    "prova = generaArrayTokenizzatoLemmi()\n",
    "print prova[2]\n",
    "prova2 = generaArrayTokenizzatoSurface()\n",
    "print prova2[2]\n",
    "arrayCompleto = generaArrayTokenizzatoSurface(listPOS = [])\n",
    "print arrayCompleto[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generaArrayNgrams(dataset, ngram=2):\n",
    "    arrayBigrammi = []\n",
    "    for item in dataset:\n",
    "        arrayBigrammi.append(list(ngrams(item,ngram)))\n",
    "    return arrayBigrammi\n",
    "\n",
    "\n",
    "arrayBigrammiLemmi = generaArrayNgrams(prova)\n",
    "arrayBigrammiSurface = generaArrayNgrams(arrayCompleto)\n",
    "\n",
    "\n",
    "arrayBigrammiChar = generaArrayNgrams(X_train)\n",
    "array3grammiChar = generaArrayNgrams(X_train, 3)\n",
    "array4grammiChar = generaArrayNgrams(X_train, 4)\n",
    "array5grammiChar = generaArrayNgrams(X_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "def importaWordSpace(pathWS = \"../ws\"):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[3:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace2(pathWS = '../DPL-IT_lrec2016.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[1:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace3(pathWS = '../emoji2vec.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace4(pathWS = '../sentix'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[3:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def generaCombWordSpace(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm != 0):\n",
    "            combDataset.append(comb/norm)\n",
    "        else:\n",
    "            combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "'''\n",
    "def getCentroide(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        i = 0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                i = i + 1\n",
    "        if(i == 0):\n",
    "            i = 1\n",
    "        combDataset.append(comb/i)\n",
    "    return combDataset\n",
    "'''\n",
    "def generaCombDPL(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace2()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(3)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm == 0):\n",
    "            norm = 1\n",
    "        combDataset.append(comb/norm)\n",
    "    return combDataset\n",
    "\n",
    "def generaCombSentix(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace4()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(4)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                try:\n",
    "                    comb = comb + embeddings_index[parola]\n",
    "                except:\n",
    "                    print parola\n",
    "                    print embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm == 0):\n",
    "            norm = 1\n",
    "        combDataset.append(comb/norm)\n",
    "    return combDataset\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def generaEmoji(dataset = X_train):\n",
    "    embeddings_index = importaWordSpace3()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(tweet)\n",
    "        comb = np.zeros(300)\n",
    "        for parola in tweettoknz:\n",
    "            if(parola in embeddings_index):\n",
    "                print parola\n",
    "                comb = comb + embeddings_index[parola]\n",
    "        combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "def feturesPunteggiatura(dataset = X_train):\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        charset = [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "        numUpperCase = 0.0\n",
    "        numchar = 0.0\n",
    "        for char in tweet:\n",
    "            numchar += 1\n",
    "            if(char.istitle()):\n",
    "                numUpperCase += 1\n",
    "            if(char == \"!\"):\n",
    "                charset[0] += 1\n",
    "            if(char == \"?\"):\n",
    "                charset[1] += 1\n",
    "            if(char == \".\"):\n",
    "                charset[2] += 1    \n",
    "            if(char == \",\"):\n",
    "                charset[3] += 1\n",
    "            if(char == \";\"):\n",
    "                charset[4] += 1\n",
    "            if(char == \":\"):\n",
    "                charset[5] += 1\n",
    "        charset.append(numUpperCase/numchar)\n",
    "        combDataset.append(charset)\n",
    "    return combDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "#Combinazione Lineare delle parole del Word Space moltiplicate per la distanza dal centroide\n",
    "#Per ogni Elemento del dataset ritorna la tupla (Bow, Var, Mean)\n",
    "def generaIronySpecificBow(dataset, arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for index, tweet in enumerate(dataset):\n",
    "        comb = np.zeros(250)\n",
    "        arrayDistanze = []\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola] * distance.cosine(embeddings_index[parola], arrayCentroide[index])\n",
    "                arrayDistanze.append(distance.euclidean(embeddings_index[parola], arrayCentroide[index]))\n",
    "        x = asarray(arrayDistanze)\n",
    "        aList = (comb, np.var(x), np.mean(x))\n",
    "        combDataset.append(tuple(aList))\n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def appendBowIronySpecific(dataset, nomerappresentazione, file_name = \"dataset.klp\", embeddings_index = importaWordSpace(), arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    combDataset = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        arrayDistanze = []\n",
    "        flag = 0\n",
    "        for word in dataset[index]:\n",
    "            #calcola distanza tra word e centroide[index]\n",
    "            if(word in embeddings_index):\n",
    "                distanza = distance.cosine(arrayCentroide[index], embeddings_index[word])\n",
    "                arrayDistanze.append(distance.cosine(arrayCentroide[index], embeddings_index[word]))\n",
    "                string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":\"+ str(distanza)+\" \"\n",
    "                flag = 1\n",
    "            else:\n",
    "                distanza = 1.0\n",
    "                \n",
    "        string = string + \"|EV| \"\n",
    "        '''\n",
    "        if(flag == 1):\n",
    "            file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        else:\n",
    "            file_lines.append(''.join([x.strip(), \"\", '\\n']))\n",
    "        '''\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        x = asarray(arrayDistanze)\n",
    "        if(np.isnan(np.var(x)) or np.isnan(np.mean(x))):\n",
    "            aList = [-1,-1]\n",
    "        else:\n",
    "            aList = [np.var(x), np.mean(x)]\n",
    "        combDataset.append(aList)\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "    \n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def appendBow(dataset, nomerappresentazione, file_name = \"dataset.klp\", tupla = True):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        for word in dataset[index]:\n",
    "            if(tupla):\n",
    "                string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "            else:\n",
    "                string = string + \"_\" + word + \":1.0 \"\n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines) \n",
    "\n",
    "def appendDenseRapr(dataset, nomerappresentazione, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        for num in dataset[index]:\n",
    "            string = string + str(num) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "\n",
    "\n",
    "def appendDenseRaprTuple(dataset, nomerappresentazione, idTupla, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        if( idTupla < 1):\n",
    "            for num in dataset[index][idTupla]:\n",
    "                string = string + str(num) + \" \"\n",
    "        else:\n",
    "            string = string + str(dataset[index][idTupla]) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "combWS = generaCombWordSpace()\n",
    "appendDenseRapr(combWS, \"WSSurface\",NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "comDPL = generaCombDPL()\n",
    "appendDenseRapr(comDPL, \"combDPL\",NOME_FILE)\n",
    "\n",
    "comSentix = generaCombSentix()\n",
    "appendDenseRapr(comSentix, \"combSentix\",NOME_FILE)\n",
    "\n",
    "combEmoji = generaEmoji()\n",
    "appendDenseRapr(combEmoji, \"combEmoji\",NOME_FILE)\n",
    "\n",
    "featuresPuntegg = feturesPunteggiatura()\n",
    "appendDenseRapr(featuresPuntegg, \"featPunt\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanVarArray = appendBowIronySpecific(prova, \"bowIronySpecific\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray, \"VarMean\",NOME_FILE)\n",
    "IronySpecificASV = generaArrayTokenizzatoSurface(listPOS = ['A','S','V'])\n",
    "meanVarArray2 = appendBowIronySpecific(IronySpecificASV, \"bowIronySpecificASV\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray2, \"VarMeanASV\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "irony_specific_bow = generaIronySpecificBow(arrayCompleto)\n",
    "appendDenseRaprTuple(irony_specific_bow, \"IronySpecificWS\", 0 ,NOME_FILE)\n",
    "#appendDenseRaprTuple(irony_specific_bow, \"Var\", 1 ,NOME_FILE)\n",
    "#appendDenseRaprTuple(irony_specific_bow, \"Mean\", 2 ,NOME_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendBow(prova, \"bowLemmi\", NOME_FILE, tupla = False)\n",
    "appendBow(arrayBigrammiLemmi, \"bowBigramLemmi\", NOME_FILE)\n",
    "appendBow(arrayBigrammiSurface, \"bowBigramSurface\", NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "appendBow(arrayBigrammiChar, \"bow2gramChar\", NOME_FILE)\n",
    "appendBow(array3grammiChar, \"bow3gramChar\", NOME_FILE)\n",
    "appendBow(array4grammiChar, \"bow4gramChar\", NOME_FILE)\n",
    "appendBow(array5grammiChar, \"bow5gramChar\", NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicator(file_name):\n",
    "    f = open(file_name, 'r').readlines()\n",
    "    d = open(\"duplicator.klp\", 'w')\n",
    "    d.writelines(f)\n",
    "    d.writelines(f)\n",
    "    file_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicator(NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
