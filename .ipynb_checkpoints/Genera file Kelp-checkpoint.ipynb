{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \n",
       "0      0        0   HSC  \n",
       "1      0        0   HSC  \n",
       "2      0        0   HSC  "
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, irony, sarcasm, topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "df[(df.irony==0) & (df.sarcasm ==1)].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>Prima figura di merda di #Trump: i musulmani con la #GreenCard possono entrare. E grazie ar cazzo!!! Ma lui, #TheDonald, non lo sapeva?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>La buona scuola in 12 punti. Ma quanti errori grammaticali ! Renzi è infastidito e cerca il GUFO SABOTATORE. http://t.co/jzPxE2UxAw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>#Roma, le amnesie della Giunta Raggi. Quanto potrà tenere nel cassetto la questione #rom? | Online-News: https://t.co/a2ZNExay8m</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>#dimartedì Comunque Bartoletti ha ragione, basta fare come Re Mitridate. Basta pesare ai rom, e vedere la loro salute.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>@fedefadeaway ma adesso arriva la buona scuola...e siamo tutti felici :-) @ComuneMI</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Maroni: \"Utilizzeremo le finestre del Pirellone per altre iniziative\". Come facevano alla questura di Milano. [@pirata_21]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>“Vincere non e importante ma e l'unica cosa che conta“ -Gianpiero Boniperti</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>Ocse: in Italia più alunni per docente, stipendi prof in calo e meno soldi per studente e @matteorenzi vuole fare #labuonascuola</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Una buona scuola per un mondo buono. Firmato Mulino Bianco</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>L'Isis giustizia i gay lanciandoli da un palazzo. Da noi aspettiamo che lo facciano da soli. [@pirata_21]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>@giusy4europe inoltre conosci luccisano? Se si chiedigli quanti giorni ha insegnato in una scuola prima di scrivere #labuonascuola</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>Ma tutte queste pubblicità di farmaci contro il bruciore di stomaco sono dovute agli stravizi natalizi o alle virtù di Mario Monti?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>Convincetemi che con governo #Monti non stiamo facendo come quelle sette che praticano suicidio di massa. In fiduciosa attesa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>Ma poi è meraviglioso vedere come in Francia sono tutti uniti contro il terrorismo ed in Italia il cattivone è Salvini! A me fate paura voi!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>#Vicenza zona S. Paolo...il sindaco #PD caccia i rom da una parte della città e \"apre le porte\" in altri posti...… https://t.co/FjI8bklJ6n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>Il Papa: \"I preservativi? Il vero problema dell'Africa è la malnutrizione\". Storica apertura all'ingoio. [@MisterDonnie13]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>#labuonascuola è un documento di 136 pag! Cazzo pretendono? Che me lo legga tutto? Domani leggo il blog di Grillo e attacco la riforma.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234</th>\n",
       "      <td>@renzieilgrullo @nomfup riforma scuola diventato un questionario on-line. Aspettiamo con ansia quello su art.18 così passano altri 6 mesi.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>@RaiStoria @Massimo_Masini Gli immigrati africani in Italia, invece, sono ospitati a oziare in alberghi a 3-4 stelle. Bella differenza.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>#labuonascuola dopo 150000 assunzioni resteranno i soldi per comprare la carta igienica? Perché al momento non ce ne sono</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              text  \\\n",
       "665        Prima figura di merda di #Trump: i musulmani con la #GreenCard possono entrare. E grazie ar cazzo!!! Ma lui, #TheDonald, non lo sapeva?   \n",
       "1308           La buona scuola in 12 punti. Ma quanti errori grammaticali ! Renzi è infastidito e cerca il GUFO SABOTATORE. http://t.co/jzPxE2UxAw   \n",
       "2653              #Roma, le amnesie della Giunta Raggi. Quanto potrà tenere nel cassetto la questione #rom? | Online-News: https://t.co/a2ZNExay8m   \n",
       "2989                        #dimartedì Comunque Bartoletti ha ragione, basta fare come Re Mitridate. Basta pesare ai rom, e vedere la loro salute.   \n",
       "3679                                                           @fedefadeaway ma adesso arriva la buona scuola...e siamo tutti felici :-) @ComuneMI   \n",
       "990                     Maroni: \"Utilizzeremo le finestre del Pirellone per altre iniziative\". Come facevano alla questura di Milano. [@pirata_21]   \n",
       "3874                                                                   “Vincere non e importante ma e l'unica cosa che conta“ -Gianpiero Boniperti   \n",
       "828               Ocse: in Italia più alunni per docente, stipendi prof in calo e meno soldi per studente e @matteorenzi vuole fare #labuonascuola   \n",
       "89                                                                                      Una buona scuola per un mondo buono. Firmato Mulino Bianco   \n",
       "1386                                     L'Isis giustizia i gay lanciandoli da un palazzo. Da noi aspettiamo che lo facciano da soli. [@pirata_21]   \n",
       "3605            @giusy4europe inoltre conosci luccisano? Se si chiedigli quanti giorni ha insegnato in una scuola prima di scrivere #labuonascuola   \n",
       "1079           Ma tutte queste pubblicità di farmaci contro il bruciore di stomaco sono dovute agli stravizi natalizi o alle virtù di Mario Monti?   \n",
       "2151                 Convincetemi che con governo #Monti non stiamo facendo come quelle sette che praticano suicidio di massa. In fiduciosa attesa   \n",
       "1095  Ma poi è meraviglioso vedere come in Francia sono tutti uniti contro il terrorismo ed in Italia il cattivone è Salvini! A me fate paura voi!   \n",
       "2602    #Vicenza zona S. Paolo...il sindaco #PD caccia i rom da una parte della città e \"apre le porte\" in altri posti...… https://t.co/FjI8bklJ6n   \n",
       "1546                    Il Papa: \"I preservativi? Il vero problema dell'Africa è la malnutrizione\". Storica apertura all'ingoio. [@MisterDonnie13]   \n",
       "2833       #labuonascuola è un documento di 136 pag! Cazzo pretendono? Che me lo legga tutto? Domani leggo il blog di Grillo e attacco la riforma.   \n",
       "3234    @renzieilgrullo @nomfup riforma scuola diventato un questionario on-line. Aspettiamo con ansia quello su art.18 così passano altri 6 mesi.   \n",
       "3244       @RaiStoria @Massimo_Masini Gli immigrati africani in Italia, invece, sono ospitati a oziare in alberghi a 3-4 stelle. Bella differenza.   \n",
       "2845                     #labuonascuola dopo 150000 assunzioni resteranno i soldi per comprare la carta igienica? Perché al momento non ce ne sono   \n",
       "\n",
       "      irony  sarcasm  \n",
       "665       1        0  \n",
       "1308      1        0  \n",
       "2653      1        1  \n",
       "2989      1        1  \n",
       "3679      1        0  \n",
       "990       1        1  \n",
       "3874      1        0  \n",
       "828       1        0  \n",
       "89        1        0  \n",
       "1386      1        0  \n",
       "3605      1        1  \n",
       "1079      1        0  \n",
       "2151      1        1  \n",
       "1095      1        1  \n",
       "2602      1        1  \n",
       "1546      1        0  \n",
       "2833      1        0  \n",
       "3234      1        0  \n",
       "3244      1        1  \n",
       "2845      1        0  "
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "df[(df.irony==1) | (df.sarcasm ==1)][['text','irony','sarcasm']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         2023\n",
       "text       2023\n",
       "irony      2023\n",
       "sarcasm    2023\n",
       "topic      2023\n",
       "dtype: int64"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.irony==1) | (df.sarcasm ==1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbashCommand = \"java -version\"\\nimport subprocess\\nprocess = subprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'], stdout=subprocess.PIPE)\\noutput, error = process.communicate()\\nprint(output)\\nprint(error)\\n\\nsubprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'])\\nfrom subprocess import check_output\\nout = check_output([\"java\", \"-version\"])\\nprint(out)\\n\\n'"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bashCommand = \"java -version\"\n",
    "import subprocess\n",
    "process = subprocess.Popen(['/bin/sh', 'java', '-version'], stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output)\n",
    "print(error)\n",
    "\n",
    "subprocess.Popen(['/bin/sh', 'java', '-version'])\n",
    "from subprocess import check_output\n",
    "out = check_output([\"java\", \"-version\"])\n",
    "print(out)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArraydataset(df=df, target=\"irony\", lower = True):\n",
    "    arrayDataset = []\n",
    "    arrayTarget = []\n",
    "    for index, row in df.iterrows():\n",
    "        if(lower):\n",
    "            arrayDataset.append(row[\"text\"].lower())\n",
    "        else:\n",
    "            arrayDataset.append(row[\"text\"])\n",
    "        if(row[target] == 0):\n",
    "            arrayTarget.append(-1)\n",
    "        else:\n",
    "            arrayTarget.append(row[target])\n",
    "    return [arrayDataset, arrayTarget]\n",
    "\n",
    "array = generaArraydataset()\n",
    "X_train = array[0]\n",
    "y_train_irony = array[1]\n",
    "y_train_sarcasm = generaArraydataset(target=\"sarcasm\")[1]\n",
    "\n",
    "'''\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "#token = nltk.word_tokenize(X_train[2])\n",
    "print(tknzr.tokenize(X_train[2]))\n",
    "'''\n",
    "print(y_train_irony[2])\n",
    "print(y_train_sarcasm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import hashlib\n",
    "\n",
    "def creaBoW(dataset, target, name = \"dataset.klp\"):\n",
    "    f = open(name,\"w+\")\n",
    "    for index, riga in enumerate(dataset):\n",
    "        string = str(target[index]) + \" |BV:bow| \"\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(dataset[index])\n",
    "        for word in tweettoknz:\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "        f.write(string + \"|EV| |BS:comment| \"+ dataset[index] +\" |ES| |BS:IDTweet| \"+ str(df.iloc[index]['id']) +\" |ES| \\n\")    \n",
    "    f.close()\n",
    "    \n",
    "NOME_FILE = \"dataset_ironia.klp\"\n",
    "#creaBoW(X_train, y_train_sarcasm, NOME_FILE)\n",
    "creaBoW(X_train, y_train_irony, NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "      <th>text::text::S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \\\n",
       "0      0        0   HSC   \n",
       "1      0        0   HSC   \n",
       "2      0        0   HSC   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                    text::text::S   \n",
       "0                                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL   \n",
       "1                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL   \n",
       "2  Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL   "
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018_renlt_processed.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArrayTokenizzatoLemmi(df=df):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    #per ogni elemento del dataset\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        #per ogni token text::text::S\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            #divido ogni singolo token e prendo il lemma\n",
    "            arrayFrase.append(arrayParola[1])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "\n",
    "def generaArrayTokenizzatoSurface(df=df, listPOS = ['A','B','S','V','HTG'], lower = True):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            if((arrayParola[2] in listPOS) and listPOS != []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "            if(listPOS == []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "    \n",
    "prova = generaArrayTokenizzatoLemmi()\n",
    "print prova[2]\n",
    "prova2 = generaArrayTokenizzatoSurface()\n",
    "print prova2[2]\n",
    "arrayCompleto = generaArrayTokenizzatoSurface(listPOS = [])\n",
    "print arrayCompleto[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generaArrayNgrams(dataset, ngram=2):\n",
    "    arrayBigrammi = []\n",
    "    for item in dataset:\n",
    "        arrayBigrammi.append(list(ngrams(item,ngram)))\n",
    "    return arrayBigrammi\n",
    "\n",
    "\n",
    "arrayBigrammiLemmi = generaArrayNgrams(prova)\n",
    "arrayBigrammiSurface = generaArrayNgrams(arrayCompleto)\n",
    "\n",
    "\n",
    "arrayBigrammiChar = generaArrayNgrams(X_train)\n",
    "array3grammiChar = generaArrayNgrams(X_train, 3)\n",
    "array4grammiChar = generaArrayNgrams(X_train, 4)\n",
    "array5grammiChar = generaArrayNgrams(X_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "def importaWordSpace(pathWS = \"../ws\"):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[3:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace2(pathWS = '../DPL-IT_lrec2016.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[1:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace3(pathWS = '../emoji2vec.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def generaCombWordSpace(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm != 0):\n",
    "            combDataset.append(comb/norm)\n",
    "        else:\n",
    "            combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "'''\n",
    "def getCentroide(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        i = 0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                i = i + 1\n",
    "        if(i == 0):\n",
    "            i = 1\n",
    "        combDataset.append(comb/i)\n",
    "    return combDataset\n",
    "'''\n",
    "def generaCombDPL(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace2()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(3)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm == 0):\n",
    "            norm = 1\n",
    "        combDataset.append(comb/norm)\n",
    "    return combDataset\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def generaEmoji(dataset = X_train):\n",
    "    embeddings_index = importaWordSpace3()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(tweet)\n",
    "        comb = np.zeros(300)\n",
    "        for parola in tweettoknz:\n",
    "            if(parola in embeddings_index):\n",
    "                print parola\n",
    "                comb = comb + embeddings_index[parola]\n",
    "        combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "def feturesPunteggiatura(dataset = X_train):\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        charset = [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "        numUpperCase = 0.0\n",
    "        numchar = 0.0\n",
    "        for char in tweet:\n",
    "            numchar += 1\n",
    "            if(char.istitle()):\n",
    "                numUpperCase += 1\n",
    "            if(char == \"!\"):\n",
    "                charset[0] += 1\n",
    "            if(char == \"?\"):\n",
    "                charset[1] += 1\n",
    "            if(char == \".\"):\n",
    "                charset[2] += 1    \n",
    "            if(char == \",\"):\n",
    "                charset[3] += 1\n",
    "            if(char == \";\"):\n",
    "                charset[4] += 1\n",
    "            if(char == \":\"):\n",
    "                charset[5] += 1\n",
    "        charset.append(numUpperCase/numchar)\n",
    "        combDataset.append(charset)\n",
    "    return combDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom scipy.spatial import distance\\n\\n#Combinazione Lineare delle parole del Word Space moltiplicate per la distanza dal centroide\\n#Per ogni Elemento del dataset ritorna la tupla (Bow, Var, Mean)\\ndef generaIronySpecificBow(dataset, arrayCentroide = getCentroide()):\\n    embeddings_index = importaWordSpace()\\n    combDataset = []\\n    for index, tweet in enumerate(dataset):\\n        comb = np.zeros(250)\\n        arrayDistanze = []\\n        for parola in tweet:\\n            if(parola in embeddings_index):\\n                comb = comb + embeddings_index[parola] * distance.euclidean(embeddings_index[parola], arrayCentroide[index])\\n                arrayDistanze.append(distance.euclidean(embeddings_index[parola], arrayCentroide[index]))\\n        x = asarray(arrayDistanze)\\n        aList = (comb, np.var(x), np.mean(x))\\n        combDataset.append(tuple(aList))\\n    return combDataset\\n'"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from scipy.spatial import distance\n",
    "\n",
    "#Combinazione Lineare delle parole del Word Space moltiplicate per la distanza dal centroide\n",
    "#Per ogni Elemento del dataset ritorna la tupla (Bow, Var, Mean)\n",
    "def generaIronySpecificBow(dataset, arrayCentroide = getCentroide()):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for index, tweet in enumerate(dataset):\n",
    "        comb = np.zeros(250)\n",
    "        arrayDistanze = []\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola] * distance.euclidean(embeddings_index[parola], arrayCentroide[index])\n",
    "                arrayDistanze.append(distance.euclidean(embeddings_index[parola], arrayCentroide[index]))\n",
    "        x = asarray(arrayDistanze)\n",
    "        aList = (comb, np.var(x), np.mean(x))\n",
    "        combDataset.append(tuple(aList))\n",
    "    return combDataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def appendBowIronySpecific(dataset, nomerappresentazione, file_name = \"dataset.klp\", embeddings_index = importaWordSpace(), arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    combDataset = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        arrayDistanze = []\n",
    "        for word in dataset[index]:\n",
    "            #calcola distanza tra word e centroide[index]\n",
    "            if(word in embeddings_index):\n",
    "                distanza = distance.cosine(arrayCentroide[index], embeddings_index[word])\n",
    "                arrayDistanze.append(distance.cosine(arrayCentroide[index], embeddings_index[word]))\n",
    "            else:\n",
    "                distanza = 1.0\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":\"+ str(distanza)+\" \"    \n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        x = asarray(arrayDistanze)\n",
    "        if(np.isnan(np.var(x)) or np.isnan(np.mean(x))):\n",
    "            aList = []\n",
    "        else:\n",
    "            aList = [np.var(x), np.mean(x)]\n",
    "        combDataset.append(aList)\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "    \n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def appendBow(dataset, nomerappresentazione, file_name = \"dataset.klp\", tupla = True):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        for word in dataset[index]:\n",
    "            if(tupla):\n",
    "                string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "            else:\n",
    "                string = string + \"_\" + word + \":1.0 \"\n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines) \n",
    "\n",
    "def appendDenseRapr(dataset, nomerappresentazione, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        for num in dataset[index]:\n",
    "            string = string + str(num) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "\n",
    "\n",
    "def appendDenseRaprTuple(dataset, nomerappresentazione, idTupla, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        if( idTupla < 1):\n",
    "            for num in dataset[index][idTupla]:\n",
    "                string = string + str(num) + \" \"\n",
    "        else:\n",
    "            string = string + str(dataset[index][idTupla]) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "combWS = generaCombWordSpace()\n",
    "appendDenseRapr(combWS, \"WSSurface\",NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "comDPL = generaCombDPL()\n",
    "appendDenseRapr(comDPL, \"combDPL\",NOME_FILE)\n",
    "\n",
    "combEmoji = generaEmoji()\n",
    "appendDenseRapr(combEmoji, \"combEmoji\",NOME_FILE)\n",
    "\n",
    "featuresPuntegg = feturesPunteggiatura()\n",
    "appendDenseRapr(featuresPuntegg, \"featPunt\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanVarArray = appendBowIronySpecific(prova, \"bowIronySpecific\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray, \"VarMean\",NOME_FILE)\n",
    "IronySpecificASV = generaArrayTokenizzatoSurface(listPOS = ['A','S','V'])\n",
    "meanVarArray2 = appendBowIronySpecific(IronySpecificASV, \"bowIronySpecificASV\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray2, \"VarMeanASV\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nirony_specific_bow = generaIronySpecificBow(arrayCompleto)\\nappendDenseRaprTuple(irony_specific_bow, \"bowIronySpecific\", 0 ,NOME_FILE)\\nappendDenseRaprTuple(irony_specific_bow, \"Var\", 1 ,NOME_FILE)\\nappendDenseRaprTuple(irony_specific_bow, \"Mean\", 2 ,NOME_FILE)\\n'"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "irony_specific_bow = generaIronySpecificBow(arrayCompleto)\n",
    "appendDenseRaprTuple(irony_specific_bow, \"bowIronySpecific\", 0 ,NOME_FILE)\n",
    "appendDenseRaprTuple(irony_specific_bow, \"Var\", 1 ,NOME_FILE)\n",
    "appendDenseRaprTuple(irony_specific_bow, \"Mean\", 2 ,NOME_FILE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendBow(prova, \"bowLemmi\", NOME_FILE, tupla = False)\n",
    "appendBow(arrayBigrammiLemmi, \"bowBigramLemmi\", NOME_FILE)\n",
    "appendBow(arrayBigrammiSurface, \"bowBigramSurface\", NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "appendBow(arrayBigrammiChar, \"bow2gramChar\", NOME_FILE)\n",
    "appendBow(array3grammiChar, \"bow3gramChar\", NOME_FILE)\n",
    "appendBow(array4grammiChar, \"bow4gramChar\", NOME_FILE)\n",
    "appendBow(array5grammiChar, \"bow5gramChar\", NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
