{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \n",
       "0      0        0   HSC  \n",
       "1      0        0   HSC  \n",
       "2      0        0   HSC  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, irony, sarcasm, topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "df[(df.irony==0) & (df.sarcasm ==1)].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>@Piovegovernolad Fossi straniera verrei anch'io in Italia. Si vive gratis. Io devo sopravvivere con l'aiuto dei miei.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>Gentiloni: la maggior parte degli islamici rifiuta il terrorismo, sono vittime https://t.co/2eo5VQmlEL il silenzio dopo gli attentati?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Mario, Monti sulla #cadrega</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>Facile dire via gli stranieri , a quelli poveri , perché a quelli ricchi gli lecchiamo il culo .gli vendiamo tutto .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Pare che il governo Monti, nella sua sobrietà e pacatezza, invierà a tutti un buono omaggio per l'acquisto di una confezione di crema Luan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>Grillo apre al Pd. Fossi in Renzi gli manderei Bersani. [@Goemon_SM]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Scandalo SAMSUNG... non usciranno anche tangenti per fornire cellulari agli immigrati appena sbarcati? Pensare male non si sbaglia mai!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>@gditom @SteGiannini #labuonascuola si assume chi ha fatto il concorso nel 99 e si lascia a casa chi lo ha fatto l'anno scorso...complimenti</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>@AFoccardi Il Papa dovrebbe essere la maggior espressione della carità cristiana,chiedetegli quanti profughi ci sono in Vaticano?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>@FusacchiA @39BorgoPinti E' come dare agli 80000 una pistola con un solo colpo da sparare: o si vive o si muore. E' la buona scuola????</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>“@matteosalvinimi: Gli #immigrati che lavorano bene sono i benvenuti. Quindi #Muntari può tornare a casa sua. #Salvini #Lega Un mito!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Gianni Morandi è talmente buono che manda tutti i profughi a prendere il latte #hashtagradio1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>@SSLAZIO1900SS @alinomilan  Centinaia di migliaia di stranieri girano sapendosi impunibili in Italia. Ringraziamo istituzioni.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>@BottaAdv ... E che gli immigrati ci pagano la pensione e che bisogna accoglierne di piu con tutti i mezzi ecc ecc ecc</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3894</th>\n",
       "      <td>“@indivanados: 700 migranti affogano ma nel frattempo vengono a galla tutti gli stronzi #MareOmnium http://t.co/vnq9sophtd”#fecciaitaliana</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>Il Parlamento europeo si riempie di euroscettici. Ma aspettate che arrivi il primo stipendio. [CONTINUA su http://t.co/oDPUtxkMK3]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>Non riescono proprio a trovarli dei \"terroristi originali\" del Medioriente. Sono tutti nati e vissuti nella nostra \"buona e sana\" Europa... chissà come mai.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>Incoerenza del PDL: Passera (competente) nel governo Monti e si grida allo scandalo, passera (incompetente) nel governo B. e tutto va bene.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>#la7 ma perche' Mario Monti non fa il premier? Che persona competente e per bene!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3488</th>\n",
       "      <td>@Maodoc55 @myrtamerlino @VittorioSgarbi Perchè il Vaticano o le diocesi  i non riempono i  Seminari vuoti con i migranti , parlano bene ma?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                               text  \\\n",
       "3269                                          @Piovegovernolad Fossi straniera verrei anch'io in Italia. Si vive gratis. Io devo sopravvivere con l'aiuto dei miei.   \n",
       "1869                         Gentiloni: la maggior parte degli islamici rifiuta il terrorismo, sono vittime https://t.co/2eo5VQmlEL il silenzio dopo gli attentati?   \n",
       "992                                                                                                                                     Mario, Monti sulla #cadrega   \n",
       "1931                                           Facile dire via gli stranieri , a quelli poveri , perché a quelli ricchi gli lecchiamo il culo .gli vendiamo tutto .   \n",
       "768                      Pare che il governo Monti, nella sua sobrietà e pacatezza, invierà a tutti un buono omaggio per l'acquisto di una confezione di crema Luan   \n",
       "1765                                                                                           Grillo apre al Pd. Fossi in Renzi gli manderei Bersani. [@Goemon_SM]   \n",
       "388                        Scandalo SAMSUNG... non usciranno anche tangenti per fornire cellulari agli immigrati appena sbarcati? Pensare male non si sbaglia mai!!   \n",
       "3636                   @gditom @SteGiannini #labuonascuola si assume chi ha fatto il concorso nel 99 e si lascia a casa chi lo ha fatto l'anno scorso...complimenti   \n",
       "3844                              @AFoccardi Il Papa dovrebbe essere la maggior espressione della carità cristiana,chiedetegli quanti profughi ci sono in Vaticano?   \n",
       "3654                        @FusacchiA @39BorgoPinti E' come dare agli 80000 una pistola con un solo colpo da sparare: o si vive o si muore. E' la buona scuola????   \n",
       "3888                          “@matteosalvinimi: Gli #immigrati che lavorano bene sono i benvenuti. Quindi #Muntari può tornare a casa sua. #Salvini #Lega Un mito!   \n",
       "1862                                                                  Gianni Morandi è talmente buono che manda tutti i profughi a prendere il latte #hashtagradio1   \n",
       "3171                                 @SSLAZIO1900SS @alinomilan  Centinaia di migliaia di stranieri girano sapendosi impunibili in Italia. Ringraziamo istituzioni.   \n",
       "3779                                         @BottaAdv ... E che gli immigrati ci pagano la pensione e che bisogna accoglierne di piu con tutti i mezzi ecc ecc ecc   \n",
       "3894                     “@indivanados: 700 migranti affogano ma nel frattempo vengono a galla tutti gli stronzi #MareOmnium http://t.co/vnq9sophtd”#fecciaitaliana   \n",
       "1541                             Il Parlamento europeo si riempie di euroscettici. Ma aspettate che arrivi il primo stipendio. [CONTINUA su http://t.co/oDPUtxkMK3]   \n",
       "844   Non riescono proprio a trovarli dei \"terroristi originali\" del Medioriente. Sono tutti nati e vissuti nella nostra \"buona e sana\" Europa... chissà come mai.    \n",
       "1466                    Incoerenza del PDL: Passera (competente) nel governo Monti e si grida allo scandalo, passera (incompetente) nel governo B. e tutto va bene.   \n",
       "2902                                                                              #la7 ma perche' Mario Monti non fa il premier? Che persona competente e per bene!   \n",
       "3488                    @Maodoc55 @myrtamerlino @VittorioSgarbi Perchè il Vaticano o le diocesi  i non riempono i  Seminari vuoti con i migranti , parlano bene ma?   \n",
       "\n",
       "      irony  sarcasm  \n",
       "3269      1        1  \n",
       "1869      1        1  \n",
       "992       1        0  \n",
       "1931      1        1  \n",
       "768       1        1  \n",
       "1765      1        1  \n",
       "388       1        1  \n",
       "3636      1        0  \n",
       "3844      1        1  \n",
       "3654      1        0  \n",
       "3888      1        1  \n",
       "1862      1        0  \n",
       "3171      1        1  \n",
       "3779      1        0  \n",
       "3894      1        0  \n",
       "1541      1        1  \n",
       "844       1        1  \n",
       "1466      1        1  \n",
       "2902      1        1  \n",
       "3488      1        1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "df[(df.irony==1) | (df.sarcasm ==1)][['text','irony','sarcasm']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         2023\n",
       "text       2023\n",
       "irony      2023\n",
       "sarcasm    2023\n",
       "topic      2023\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.irony==1) | (df.sarcasm ==1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbashCommand = \"java -version\"\\nimport subprocess\\nprocess = subprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'], stdout=subprocess.PIPE)\\noutput, error = process.communicate()\\nprint(output)\\nprint(error)\\n\\nsubprocess.Popen([\\'/bin/sh\\', \\'java\\', \\'-version\\'])\\nfrom subprocess import check_output\\nout = check_output([\"java\", \"-version\"])\\nprint(out)\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bashCommand = \"java -version\"\n",
    "import subprocess\n",
    "process = subprocess.Popen(['/bin/sh', 'java', '-version'], stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output)\n",
    "print(error)\n",
    "\n",
    "subprocess.Popen(['/bin/sh', 'java', '-version'])\n",
    "from subprocess import check_output\n",
    "out = check_output([\"java\", \"-version\"])\n",
    "print(out)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArraydataset(df=df, target=\"irony\", lower = True):\n",
    "    arrayDataset = []\n",
    "    arrayTarget = []\n",
    "    for index, row in df.iterrows():\n",
    "        if(lower):\n",
    "            arrayDataset.append(row[\"text\"].lower())\n",
    "        else:\n",
    "            arrayDataset.append(row[\"text\"])\n",
    "        if(row[target] == 0):\n",
    "            arrayTarget.append(-1)\n",
    "        else:\n",
    "            arrayTarget.append(row[target])\n",
    "    return [arrayDataset, arrayTarget]\n",
    "\n",
    "array = generaArraydataset()\n",
    "X_train = array[0]\n",
    "y_train_irony = array[1]\n",
    "y_train_sarcasm = generaArraydataset(target=\"sarcasm\")[1]\n",
    "\n",
    "'''\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "#token = nltk.word_tokenize(X_train[2])\n",
    "print(tknzr.tokenize(X_train[2]))\n",
    "'''\n",
    "print(y_train_irony[2])\n",
    "print(y_train_sarcasm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import hashlib\n",
    "\n",
    "def creaBoW(dataset, target, name = \"dataset.klp\"):\n",
    "    f = open(name,\"w+\")\n",
    "    for index, riga in enumerate(dataset):\n",
    "        string = str(target[index]) + \" |BV:bow| \"\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(dataset[index])\n",
    "        for word in tweettoknz:\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "        f.write(string + \"|EV| |BS:comment| \"+ dataset[index] +\" |ES| |BS:IDTweet| \"+ str(df.iloc[index]['id']) +\" |ES| \\n\")    \n",
    "    f.close()\n",
    "\n",
    "\n",
    "    \n",
    "def creaBoW2(df=df, name = \"dataset.klp\"):\n",
    "    f = open(name,\"w+\")\n",
    "    for index, row in df.iterrows():\n",
    "        if(row[\"irony\"] == 1):\n",
    "            ironia = \"Irony\"\n",
    "        else:\n",
    "            ironia = \"NOTIrony\"\n",
    "           \n",
    "        if(row[\"sarcasm\"] == 1):\n",
    "            sarcasmo = \"Sarcasmo\"\n",
    "        else:\n",
    "            sarcasmo = \"NOTSarcasmo\"\n",
    "        string = ironia + \" \" + sarcasmo + \" |BV:bow| \"\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(row[\"text\"])\n",
    "        for word in tweettoknz:\n",
    "            string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "        #f.write(string + \"|EV| |BS:comment| \"+ row[\"text\"] +\" |ES| |BS:IDTweet| \"+ str(row[\"id\"]) +\" |ES| \\n\")\n",
    "        f.write(string + \"|EV| |BS:IDTweet| \"+ str(row[\"id\"]) +\" |ES| \\n\") \n",
    "    f.close()\n",
    "    \n",
    "\n",
    "NOME_FILE = \"dataset_ironia.klp\"\n",
    "#creaBoW(X_train, y_train_irony, NOME_FILE)\n",
    "creaBoW2(name = NOME_FILE)\n",
    "#NOME_FILE = \"dataset_sarcasmo.klp\"\n",
    "#creaBoW(X_train, y_train_sarcasm, NOME_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>irony</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>topic</th>\n",
       "      <th>text::text::S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811156813181841408</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811183087350595584</td>\n",
       "      <td>Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>826380632376881152</td>\n",
       "      <td>Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HSC</td>\n",
       "      <td>Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  811156813181841408   \n",
       "1  811183087350595584   \n",
       "2  826380632376881152   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                             Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico #20dicembre https://t.co/rBjvUi8RJ2   \n",
       "1       Zurigo, trovato morto il presunto autore della sparatoria nel centro islamico - https://t.co/XMrEqPnV6w tramite https://t.co/zAAl3RtO5F   \n",
       "2  Zingari..i soliti \"MERDOSI\"..#cacciamolivia Roma, i rom aggrediscono un 81enne per rapinarlo. Bloccati dai cittadini https://t.co/j6LYylIkhe   \n",
       "\n",
       "   irony  sarcasm topic  \\\n",
       "0      0        0   HSC   \n",
       "1      0        0   HSC   \n",
       "2      0        0   HSC   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                    text::text::S   \n",
       "0                                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A #20dicembre::#20dicembre::HTG LINK::link::$URL   \n",
       "1                                                                                                 Zurigo::zurigo::SP ,::,::FF trovato::trovare::V morto::morire::V il::il::RD presunto::presumere::V autore::autore::S della::di::EA sparatoria::sparatoria::S nel::in::EA centro::centro::S islamico::islamico::A -::-::FC LINK::link::$URL tramite::tramite::S LINK::link::$URL   \n",
       "2  Zingari::zingari::SP .::.::FF i::i::RD soliti::solito::S \"::\"::FB MERDOSI::merdosi::SP \"::\"::FB .::.::FF #cacciamolivia::#cacciamolivia::HTG Roma::roma::S ,::,::FF i::i::RD rom::rom::S aggrediscono::aggredire::V un::un::RI 81::81::N enne::enne::S per::per::E rapinarlo::rapinarlo::V .::.::FF Bloccati::bloccare::V dai::da::EA cittadini::cittadino::S LINK::link::$URL   "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../training_ironita2018_renlt_processed.tsv\", sep='\\t')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaArrayTokenizzatoLemmi(df=df):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    #per ogni elemento del dataset\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        #per ogni token text::text::S\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            #divido ogni singolo token e prendo il lemma\n",
    "            arrayFrase.append(arrayParola[1])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "\n",
    "def generaArrayTokenizzatoSurface(df=df, listPOS = ['A','B','S','V','HTG'], lower = True):\n",
    "    arrayLemmi = []\n",
    "    for index, row in df.iterrows():\n",
    "        arrayLemmi.append(row[\"text::text::S \"])\n",
    "    arrayTokenizzatoLemmi = []\n",
    "    for text in arrayLemmi:\n",
    "        arrayParoleLemm = text.split()\n",
    "        arrayFrase = []\n",
    "        for parola in arrayParoleLemm:\n",
    "            arrayParola = parola.split(\"::\")\n",
    "            if((arrayParola[2] in listPOS) and listPOS != []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "            if(listPOS == []):\n",
    "                if(lower):\n",
    "                    arrayFrase.append(arrayParola[0].lower())\n",
    "                else:\n",
    "                    arrayFrase.append(arrayParola[0])\n",
    "        arrayTokenizzatoLemmi.append(arrayFrase)\n",
    "    return arrayTokenizzatoLemmi\n",
    "    \n",
    "prova = generaArrayTokenizzatoLemmi()\n",
    "print prova[2]\n",
    "prova2 = generaArrayTokenizzatoSurface()\n",
    "print prova2[2]\n",
    "arrayCompleto = generaArrayTokenizzatoSurface(listPOS = [])\n",
    "print arrayCompleto[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generaArrayNgrams(dataset, ngram=2):\n",
    "    arrayBigrammi = []\n",
    "    for item in dataset:\n",
    "        arrayBigrammi.append(list(ngrams(item,ngram)))\n",
    "    return arrayBigrammi\n",
    "\n",
    "\n",
    "arrayBigrammiLemmi = generaArrayNgrams(prova)\n",
    "arrayBigrammiSurface = generaArrayNgrams(arrayCompleto)\n",
    "\n",
    "\n",
    "arrayBigrammiChar = generaArrayNgrams(X_train)\n",
    "array3grammiChar = generaArrayNgrams(X_train, 3)\n",
    "array4grammiChar = generaArrayNgrams(X_train, 4)\n",
    "array5grammiChar = generaArrayNgrams(X_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "\n",
    "def importaWordSpace(pathWS = \"../ws\"):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[3:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace2(pathWS = '../DPL-IT_lrec2016.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        #print(word)\n",
    "        string = ''.join(values[1:]).split(',')\n",
    "        for x in string:\n",
    "            x = float(x)\n",
    "        #print(string)\n",
    "        coefs = asarray(string, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace3(pathWS = '../emoji2vec.txt'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def importaWordSpace4(pathWS = '../sentix'):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    f = open(pathWS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[3:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def generaCombWordSpace(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm != 0):\n",
    "            combDataset.append(comb/norm)\n",
    "        else:\n",
    "            combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "'''\n",
    "def getCentroide(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(250)\n",
    "        i = 0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola]\n",
    "                i = i + 1\n",
    "        if(i == 0):\n",
    "            i = 1\n",
    "        combDataset.append(comb/i)\n",
    "    return combDataset\n",
    "'''\n",
    "def generaCombDPL(dataset = generaArrayTokenizzatoSurface(listPOS = ['A','S','V'])):\n",
    "    embeddings_index = importaWordSpace2()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = [0.0]\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = np.concatenate((comb, embeddings_index[parola]), axis=None)\n",
    "        combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "def appendBowDPL( nomerappresentazione, dataset = generaArrayTokenizzatoSurface(listPOS = ['A','S','V']), file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    embeddings_index = importaWordSpace2()\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        for word in dataset[index]:\n",
    "            try:\n",
    "                vecPol = embeddings_index[word]\n",
    "            except:\n",
    "                vecPol = [0, 0, 0]\n",
    "            pal = hashlib.md5(''.join(word)).hexdigest()\n",
    "            string = string + \"_\" + pal +\"_pos:\"+ str(vecPol[0]) + \" \" + pal +\"_neg:\"+ str(vecPol[1]) + \" \" + pal +\"_net:\"+ str(vecPol[2]) + \" \"\n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines) \n",
    "\n",
    "def generaCombSentix(dataset = prova2):\n",
    "    embeddings_index = importaWordSpace4()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        comb = np.zeros(4)\n",
    "        norm = 0.0\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                try:\n",
    "                    comb = comb + embeddings_index[parola]\n",
    "                except:\n",
    "                    print parola\n",
    "                    print embeddings_index[parola]\n",
    "                norm += 1\n",
    "        if(norm == 0):\n",
    "            norm = 1\n",
    "        combDataset.append(comb/norm)\n",
    "    return combDataset\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def generaEmoji(dataset = X_train):\n",
    "    embeddings_index = importaWordSpace3()\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        tknzr = TweetTokenizer(strip_handles=True)\n",
    "        tweettoknz = tknzr.tokenize(tweet)\n",
    "        comb = np.zeros(300)\n",
    "        for parola in tweettoknz:\n",
    "            if(parola in embeddings_index):\n",
    "                print parola\n",
    "                comb = comb + embeddings_index[parola]\n",
    "        combDataset.append(comb)\n",
    "    return combDataset\n",
    "\n",
    "def feturesPunteggiatura(dataset = X_train):\n",
    "    combDataset = []\n",
    "    for tweet in dataset:\n",
    "        charset = [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "        numUpperCase = 0.0\n",
    "        numchar = 0.0\n",
    "        for char in tweet:\n",
    "            numchar += 1\n",
    "            if(char.istitle()):\n",
    "                numUpperCase += 1\n",
    "            if(char == \"!\"):\n",
    "                charset[0] += 1\n",
    "            if(char == \"?\"):\n",
    "                charset[1] += 1\n",
    "            if(char == \".\"):\n",
    "                charset[2] += 1    \n",
    "            if(char == \",\"):\n",
    "                charset[3] += 1\n",
    "            if(char == \";\"):\n",
    "                charset[4] += 1\n",
    "            if(char == \":\"):\n",
    "                charset[5] += 1\n",
    "        charset.append(numUpperCase/numchar)\n",
    "        combDataset.append(charset)\n",
    "    return combDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "#Combinazione Lineare delle parole del Word Space moltiplicate per la distanza dal centroide\n",
    "#Per ogni Elemento del dataset ritorna la tupla (Bow, Var, Mean)\n",
    "def generaIronySpecificBow(dataset, arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    embeddings_index = importaWordSpace()\n",
    "    combDataset = []\n",
    "    for index, tweet in enumerate(dataset):\n",
    "        comb = np.zeros(250)\n",
    "        arrayDistanze = []\n",
    "        for parola in tweet:\n",
    "            if(parola in embeddings_index):\n",
    "                comb = comb + embeddings_index[parola] * distance.cosine(embeddings_index[parola], arrayCentroide[index])\n",
    "                arrayDistanze.append(distance.euclidean(embeddings_index[parola], arrayCentroide[index]))\n",
    "        x = asarray(arrayDistanze)\n",
    "        aList = (comb, np.var(x), np.mean(x))\n",
    "        combDataset.append(tuple(aList))\n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def appendBowIronySpecific(dataset, nomerappresentazione, file_name = \"dataset.klp\", embeddings_index = importaWordSpace(), arrayCentroide = generaCombWordSpace(dataset=prova)):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    combDataset = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        arrayDistanze = []\n",
    "        flag = 0\n",
    "        for word in dataset[index]:\n",
    "            #calcola distanza tra word e centroide[index]\n",
    "            if(word in embeddings_index):\n",
    "                distanza = distance.cosine(arrayCentroide[index], embeddings_index[word])\n",
    "                arrayDistanze.append(1 - distance.cosine(arrayCentroide[index], embeddings_index[word]))\n",
    "                string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":\"+ str(distanza)+\" \"\n",
    "                flag = 1\n",
    "            else:\n",
    "                distanza = 1.0\n",
    "                \n",
    "        string = string + \"|EV| \"\n",
    "        '''\n",
    "        if(flag == 1):\n",
    "            file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        else:\n",
    "            file_lines.append(''.join([x.strip(), \"\", '\\n']))\n",
    "        '''\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        x = asarray(arrayDistanze)\n",
    "        if(np.isnan(np.var(x)) or np.isnan(np.mean(x))):\n",
    "            aList = [-1,-1]\n",
    "        else:\n",
    "            aList = [np.var(x), np.mean(x)]\n",
    "        combDataset.append(aList)\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "    \n",
    "    return combDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def appendBow(dataset, nomerappresentazione, file_name = \"dataset.klp\", tupla = True):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BV:\"+ nomerappresentazione + \"| \"\n",
    "        for word in dataset[index]:\n",
    "            if(tupla):\n",
    "                string = string + \"_\" + hashlib.md5(''.join(word)).hexdigest() + \":1.0 \"\n",
    "            else:\n",
    "                string = string + \"_\" + word + \":1.0 \"\n",
    "        string = string + \"|EV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines) \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def appendDenseRapr(dataset, nomerappresentazione, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        for num in dataset[index]:\n",
    "            string = string + str(num) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "\n",
    "\n",
    "def appendDenseRaprTuple(dataset, nomerappresentazione, idTupla, file_name = \"dataset.klp\"):\n",
    "    f = open(file_name, 'r')\n",
    "    file_lines = []\n",
    "    for index, x in enumerate(f.readlines()):\n",
    "        string = \" |BDV:\"+ nomerappresentazione + \"| \"\n",
    "        if( idTupla < 1):\n",
    "            for num in dataset[index][idTupla]:\n",
    "                string = string + str(num) + \" \"\n",
    "        else:\n",
    "            string = string + str(dataset[index][idTupla]) + \" \"\n",
    "        string = string + \"|EDV| \"\n",
    "        file_lines.append(''.join([x.strip(), string, '\\n']))\n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "\n",
    "        \n",
    "def padding(combDPL):\n",
    "    massima = 0\n",
    "    nuovoarray = []\n",
    "    for array in combDPL:\n",
    "        if len(array) > massima:\n",
    "            massima = len(array)\n",
    "    for array in combDPL:\n",
    "        arrayzeri = np.zeros(massima - len(array))\n",
    "        array2 = np.concatenate((array, arrayzeri), axis=None)\n",
    "        nuovoarray.append(array2)\n",
    "    return nuovoarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "combWS = generaCombWordSpace()\n",
    "appendDenseRapr(combWS, \"WSSurface\",NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "comDPL = generaCombDPL()\n",
    "comDPL = padding(comDPL)\n",
    "appendDenseRapr(comDPL, \"combDPL\",NOME_FILE)\n",
    "\n",
    "comSentix = generaCombSentix()\n",
    "appendDenseRapr(comSentix, \"combSentix\",NOME_FILE)\n",
    "\n",
    "combEmoji = generaEmoji()\n",
    "appendDenseRapr(combEmoji, \"combEmoji\",NOME_FILE)\n",
    "\n",
    "featuresPuntegg = feturesPunteggiatura()\n",
    "appendDenseRapr(featuresPuntegg, \"featPunt\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanVarArray = appendBowIronySpecific(prova, \"bowIronySpecific\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray, \"VarMean\",NOME_FILE)\n",
    "IronySpecificASV = generaArrayTokenizzatoSurface(listPOS = ['A','S','V'])\n",
    "meanVarArray2 = appendBowIronySpecific(IronySpecificASV, \"bowIronySpecificASV\", NOME_FILE)\n",
    "appendDenseRapr(meanVarArray2, \"VarMeanASV\",NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "irony_specific_bow = generaIronySpecificBow(arrayCompleto)\n",
    "appendDenseRaprTuple(irony_specific_bow, \"IronySpecificWS\", 0 ,NOME_FILE)\n",
    "#appendDenseRaprTuple(irony_specific_bow, \"Var\", 1 ,NOME_FILE)\n",
    "#appendDenseRaprTuple(irony_specific_bow, \"Mean\", 2 ,NOME_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rapinarlo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1c431f1c9b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mappendBow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprova\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bowLemmi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOME_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mappendBowDPL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnomerappresentazione\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bowDPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOME_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mappendBow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrayBigrammiLemmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bowBigramLemmi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOME_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mappendBow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrayBigrammiSurface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bowBigramSurface\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOME_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-ed23c4514972>\u001b[0m in \u001b[0;36mappendBowDPL\u001b[0;34m(nomerappresentazione, dataset, file_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" |BV:\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mnomerappresentazione\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"| \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mvecPol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpal\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_pos:\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecPol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpal\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_neg:\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecPol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpal\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_net:\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecPol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rapinarlo'"
     ]
    }
   ],
   "source": [
    "appendBow(prova, \"bowLemmi\", NOME_FILE, tupla = False)\n",
    "appendBowDPL(nomerappresentazione = \"bowDPL\", file_name = NOME_FILE)\n",
    "appendBow(arrayBigrammiLemmi, \"bowBigramLemmi\", NOME_FILE)\n",
    "appendBow(arrayBigrammiSurface, \"bowBigramSurface\", NOME_FILE)\n",
    "\n",
    "\n",
    "\n",
    "appendBow(arrayBigrammiChar, \"bow2gramChar\", NOME_FILE)\n",
    "appendBow(array3grammiChar, \"bow3gramChar\", NOME_FILE)\n",
    "appendBow(array4grammiChar, \"bow4gramChar\", NOME_FILE)\n",
    "appendBow(array5grammiChar, \"bow5gramChar\", NOME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print comDPL[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
